{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a94fd79f-8d40-460a-b760-6c8f58734111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96222851-63b9-4e11-b33f-f51f26a813de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(nn.Module):\n",
    "    def __init__(self, emb_dim, feat_num):\n",
    "        super(FM, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        #定义三个矩阵，一个是全局偏置，一个是一阶权重矩阵，一个是二阶交叉矩阵\n",
    "        self.w0 = nn.Parameter(torch.rand([1,]))\n",
    "        self.w1 = nn.Parameter(torch.rand([feat_num, 1]))\n",
    "        self.w2 = nn.Parameter(torch.rand([feat_num, emb_dim]))\n",
    "    def forward(self, x):\n",
    "        #x的维度是(batch_size, feat_num)\n",
    "        #一阶交叉\n",
    "        first_order = self.w0 + torch.mm(x, self.w1) #(batch_size, 1)\n",
    "        #二阶交叉\n",
    "        second_order = 0.5 * torch.sum(torch.pow(torch.mm(x, self.w2), 2) - torch.mm(torch.pow(x, 2), torch.pow(self.w2, 2)), dim=1, keepdim=True)\n",
    "        return torch.sigmoid(first_order + second_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b9669c4-c65b-4fee-a93b-d3595ad8d32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'feat': 'C1', 'feat_num': 79, 'embed_dim': 8}, {'feat': 'C2', 'feat_num': 252, 'embed_dim': 8}, {'feat': 'C3', 'feat_num': 1293, 'embed_dim': 8}, {'feat': 'C4', 'feat_num': 1043, 'embed_dim': 8}, {'feat': 'C5', 'feat_num': 30, 'embed_dim': 8}, {'feat': 'C6', 'feat_num': 7, 'embed_dim': 8}, {'feat': 'C7', 'feat_num': 1164, 'embed_dim': 8}, {'feat': 'C8', 'feat_num': 39, 'embed_dim': 8}, {'feat': 'C9', 'feat_num': 2, 'embed_dim': 8}, {'feat': 'C10', 'feat_num': 908, 'embed_dim': 8}, {'feat': 'C11', 'feat_num': 926, 'embed_dim': 8}, {'feat': 'C12', 'feat_num': 1239, 'embed_dim': 8}, {'feat': 'C13', 'feat_num': 824, 'embed_dim': 8}, {'feat': 'C14', 'feat_num': 20, 'embed_dim': 8}, {'feat': 'C15', 'feat_num': 819, 'embed_dim': 8}, {'feat': 'C16', 'feat_num': 1159, 'embed_dim': 8}, {'feat': 'C17', 'feat_num': 9, 'embed_dim': 8}, {'feat': 'C18', 'feat_num': 534, 'embed_dim': 8}, {'feat': 'C19', 'feat_num': 201, 'embed_dim': 8}, {'feat': 'C20', 'feat_num': 4, 'embed_dim': 8}, {'feat': 'C21', 'feat_num': 1204, 'embed_dim': 8}, {'feat': 'C22', 'feat_num': 7, 'embed_dim': 8}, {'feat': 'C23', 'feat_num': 12, 'embed_dim': 8}, {'feat': 'C24', 'feat_num': 729, 'embed_dim': 8}, {'feat': 'C25', 'feat_num': 33, 'embed_dim': 8}, {'feat': 'C26', 'feat_num': 554, 'embed_dim': 8}]]\n",
      "(1439, 27) (160, 27) (400, 27)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./data/Criteo_sample.txt')\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=2020)\n",
    "\n",
    "train_label = train_set['label']\n",
    "del train_set['label']\n",
    "test_label = test_set['label']\n",
    "del test_set['label']\n",
    "data_df = pd.concat((train_set, test_set))\n",
    "\n",
    "\n",
    "sparse_feas = [col for col in data_df.columns if col[0] == 'C']\n",
    "dense_feas = [col for col in data_df.columns if col[0] == 'I']\n",
    "\n",
    "data_df[sparse_feas] = data_df[sparse_feas].fillna('-1')\n",
    "data_df.drop(columns=dense_feas, inplace=True)\n",
    "\n",
    "def sparseFeature(feat, feat_num, embed_dim=4):\n",
    "    return {'feat':feat, 'feat_num':feat_num, 'embed_dim':embed_dim}\n",
    "def denseFeature(feat):\n",
    "    return {'feat':feat}\n",
    "embed_dim = 8\n",
    "feature_columns = [[sparseFeature(feat, len(data_df[feat].unique()), embed_dim=embed_dim) for feat in sparse_feas]]\n",
    "\n",
    "for feat in sparse_feas:\n",
    "    le = LabelEncoder()\n",
    "    data_df[feat] = le.fit_transform(data_df[feat])\n",
    "\n",
    "train = data_df[:train_set.shape[0]]\n",
    "test = data_df[train_set.shape[0]:]\n",
    "\n",
    "train['label'] = train_label\n",
    "test['label'] = test_label\n",
    "\n",
    "train_set, val_set = train_test_split(train, test_size=0.1, random_state=2024)\n",
    "\n",
    "train_set.reset_index(drop=True, inplace=True)\n",
    "val_set.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "print(feature_columns)\n",
    "print(train_set.shape, val_set.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97100312-c36f-4b01-9ee4-8c87986bf220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FM()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 4\n",
    "feat_num = train_set.shape[1] - 1\n",
    "model = FM(emb_dim, feat_num)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4295ff51-777a-4936-93f5-d0b67cb3223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_x = train_set.drop('label', axis=1).values\n",
    "trn_y = train_set['label'].values\n",
    "val_x = val_set.drop('label', axis=1).values\n",
    "val_y = val_set['label'].values\n",
    "dl_train_dataset = TensorDataset(torch.tensor(trn_x).float(), torch.tensor(trn_y).float())\n",
    "dl_val_dataset = TensorDataset(torch.tensor(val_x).float(), torch.tensor(val_y).float())\n",
    "\n",
    "dl_train = DataLoader(dl_train_dataset, shuffle=True, batch_size=32)\n",
    "dl_val = DataLoader(dl_val_dataset, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b615d70-24f6-4b96-9c6e-769ac7fa4cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_training.........\n",
      "================================================================\n",
      "[step=10] loss: 81.875, auc: 0.500\n",
      "[step=20] loss: 80.156, auc: 0.500\n",
      "[step=30] loss: 78.958, auc: 0.500\n",
      "[step=40] loss: 79.297, auc: 0.500\n",
      "\n",
      "EPOCH=1, loss=79.433, auc = 0.500, val_loss=80.000, val_auc = 0.500\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 80.000, auc: 0.500\n",
      "[step=20] loss: 78.750, auc: 0.500\n",
      "[step=30] loss: 78.646, auc: 0.500\n",
      "[step=40] loss: 79.688, auc: 0.500\n",
      "\n",
      "EPOCH=2, loss=79.422, auc = 0.500, val_loss=80.000, val_auc = 0.500\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 75.938, auc: 0.500\n",
      "[step=20] loss: 78.594, auc: 0.500\n",
      "[step=30] loss: 79.896, auc: 0.500\n",
      "[step=40] loss: 79.609, auc: 0.500\n",
      "\n",
      "EPOCH=3, loss=79.427, auc = 0.500, val_loss=80.000, val_auc = 0.500\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 79.688, auc: 0.500\n",
      "[step=20] loss: 80.469, auc: 0.500\n",
      "[step=30] loss: 80.000, auc: 0.500\n",
      "[step=40] loss: 79.141, auc: 0.500\n",
      "\n",
      "EPOCH=4, loss=79.422, auc = 0.500, val_loss=80.000, val_auc = 0.500\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 81.250, auc: 0.500\n",
      "[step=20] loss: 80.156, auc: 0.500\n",
      "[step=30] loss: 78.750, auc: 0.500\n",
      "[step=40] loss: 78.750, auc: 0.500\n",
      "\n",
      "EPOCH=5, loss=79.431, auc = 0.500, val_loss=80.000, val_auc = 0.500\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 81.250, auc: 0.500\n",
      "[step=20] loss: 80.938, auc: 0.500\n",
      "[step=30] loss: 80.104, auc: 0.500\n",
      "[step=40] loss: 79.297, auc: 0.500\n",
      "\n",
      "EPOCH=6, loss=79.427, auc = 0.500, val_loss=80.000, val_auc = 0.500\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 81.250, auc: 0.500\n",
      "[step=20] loss: 80.938, auc: 0.500\n",
      "[step=30] loss: 79.271, auc: 0.500\n",
      "[step=40] loss: 79.062, auc: 0.500\n",
      "\n",
      "EPOCH=7, loss=79.429, auc = 0.500, val_loss=80.000, val_auc = 0.500\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 77.812, auc: 0.500\n",
      "[step=20] loss: 78.750, auc: 0.500\n",
      "[step=30] loss: 78.958, auc: 0.500\n",
      "[step=40] loss: 78.438, auc: 0.500\n",
      "\n",
      "EPOCH=8, loss=79.435, auc = 0.500, val_loss=80.000, val_auc = 0.500\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 77.188, auc: 0.500\n",
      "[step=20] loss: 79.375, auc: 0.500\n",
      "[step=30] loss: 80.000, auc: 0.500\n",
      "[step=40] loss: 79.453, auc: 0.500\n",
      "\n",
      "EPOCH=9, loss=79.431, auc = 0.500, val_loss=80.000, val_auc = 0.500\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 77.500, auc: 0.500\n",
      "[step=20] loss: 79.688, auc: 0.500\n",
      "[step=30] loss: 79.271, auc: 0.500\n",
      "[step=40] loss: 78.906, auc: 0.500\n",
      "\n",
      "EPOCH=10, loss=79.429, auc = 0.500, val_loss=80.000, val_auc = 0.500\n",
      "\n",
      "================================================================================\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "def auc(y_pred, y_true):\n",
    "    pred = y_pred.data\n",
    "    y = y_true.data\n",
    "    return roc_auc_score(y, pred)\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "metric_func = auc\n",
    "metric_name = 'auc'\n",
    "epochs = 10\n",
    "log_step_freq = 10\n",
    "\n",
    "dfhistory = pd.DataFrame(columns=['epoch', 'loss', metric_name, 'val_loss', 'val_'+metric_name])\n",
    "\n",
    "print('start_training.........')\n",
    "\n",
    "print('========'*8)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "    metric_sum = 0.0\n",
    "    step = 1\n",
    "    \n",
    "\n",
    "    \n",
    "    for step, (features, labels) in enumerate(dl_train, 1):\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        labels = labels.view(-1, 1)\n",
    "        \n",
    "        # 正向传播\n",
    "        predictions = model(features);\n",
    "        loss = loss_func(predictions, labels)\n",
    "        try:\n",
    "            metric = metric_func(predictions, labels)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印batch级别日志\n",
    "        loss_sum += loss.item()\n",
    "        metric_sum += metric.item()\n",
    "        if step % log_step_freq == 0:\n",
    "            print((\"[step=%d] loss: %.3f, \" + metric_name + \": %.3f\") % (step, loss_sum/step, metric_sum/step));\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    val_metric_sum = 0.0\n",
    "    val_step = 1\n",
    "    \n",
    "    for val_step, (features, labels) in enumerate(dl_val, 1):\n",
    "        labels = labels.view(-1, 1)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(features)\n",
    "            val_loss = loss_func(predictions, labels)\n",
    "            try:\n",
    "                val_metric = metric_func(predictions, labels)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        val_loss_sum += val_loss.item()\n",
    "        val_metric_sum += val_metric.item()\n",
    "    \n",
    "    # 记录日志\n",
    "    info = (epoch, loss_sum/step, metric_sum/step, val_loss_sum/val_step, val_metric_sum/val_step)\n",
    "    dfhistory.loc[epoch-1] = info\n",
    "    \n",
    "    # 打印日志\n",
    "    print((\"\\nEPOCH=%d, loss=%.3f, \" + metric_name + \" = %.3f, val_loss=%.3f, \" + \"val_\" + metric_name + \" = %.3f\") %info)\n",
    "    print('\\n' + '=========='* 8)\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65fa5225-41c4-4163-b335-a676140e3494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc:0.500 test_acc:0.225\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_x = test.drop('label', axis=1).values\n",
    "test_y = test['label'].values\n",
    "y_pred_probs = model(torch.tensor(test_x).float())\n",
    "y_pred = torch.where(y_pred_probs>0.5, torch.ones_like(y_pred_probs), torch.zeros_like(y_pred_probs))\n",
    "\n",
    "test_auc = roc_auc_score(test_y, y_pred_probs.data.numpy())\n",
    "test_acc = accuracy_score(test_y, y_pred.data.numpy())\n",
    "print('test_auc:%.3f test_acc:%.3f'%(test_auc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f706d-2775-4143-a513-86cfc869d627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a893915-dca6-4020-b5f7-584df4d392a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
