{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51120511-c5ec-4422-aad4-a16e1fe952b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import datetime\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27e90a08-f9bc-4046-bd38-00d3e3868a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dnn(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout=0.):\n",
    "        super(Dnn, self).__init__()\n",
    "        self.dnn_network = nn.ModuleList([nn.Linear(layer[0], layer[1]) for layer in list(zip(hidden_units[:-1], hidden_units[1:]))])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        for linear in self.dnn_network:\n",
    "            x = linear(x)\n",
    "            x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "        \n",
    "class Attention_layer(nn.Module):\n",
    "    def __init__(self, att_units):\n",
    "        \"\"\"\n",
    "        att_units:[embed_dim, att_vector]\n",
    "        \"\"\"\n",
    "        super(Attention_layer, self).__init__()\n",
    "        self.att_w = nn.Linear(att_units[0], att_units[1])\n",
    "        self.att_dense = nn.Linear(att_units[1], 1)\n",
    "    def forward(self, bi_interation):\n",
    "        a = self.att_w(bi_interation)  #bi_iteation(None, field_num*(field_num-1)/2, embed_dim)\n",
    "        a = F.relu(a)\n",
    "        att_scores = self.att_dense(a) #（None，field_num*(field_num-1)/2, 1)\n",
    "        att_weight = F.softmax(att_scores, dim=1) #(None, field_num*(field_num-1)/2, 1)\n",
    "        att_out = torch.sum(att_weight * bi_interation, dim=1)\n",
    "        return att_out\n",
    "        \n",
    "class AFM(nn.Module):\n",
    "    def __init__(self, feature_columns, mode, hidden_units, att_vector=8, dropout=0.5, useDNN=False):\n",
    "        \"\"\"\n",
    "        feature_columns:特征信息\n",
    "        mode:三种模式,'max':max pooling, 'avg':average pooling, 'att':attention\n",
    "        att_vector:注意力网络的隐藏层单元个数\n",
    "        hidden_units:DNN网络的隐藏单元个数\n",
    "        dropout:Dropout比率\n",
    "        useDNN:默认不使用DNN网络\n",
    "        \"\"\"\n",
    "        super(AFM, self).__init__()\n",
    "        self.dense_feature_cols, self.sparse_feature_cols = feature_columns\n",
    "        self.mode = mode\n",
    "        self.useDNN = useDNN\n",
    "        \n",
    "        #embedding\n",
    "        self.embed_layers = nn.ModuleDict({\n",
    "            'embed_'+str(i):nn.Embedding(num_embeddings=feat['feat_num'], embedding_dim=feat['embed_dim']) for i, feat in enumerate(self.sparse_feature_cols)\n",
    "        })\n",
    "        \n",
    "        #如果这里是注意力机制的话，这里需要加一个注意力网络\n",
    "        if self.mode == 'att':\n",
    "            self.attention = Attention_layer([self.sparse_feature_cols[0]['embed_dim'], att_vector])\n",
    "        \n",
    "        #如果使用DNN的话，这里需要初始化DNN网络\n",
    "        if self.useDNN:\n",
    "            self.fea_num = len(self.dense_feature_cols) + self.sparse_feature_cols[0]['embed_dim']\n",
    "            hidden_units.insert(0, self.fea_num)\n",
    "            \n",
    "            self.bn = nn.BatchNorm1d(self.fea_num)\n",
    "            self.dnn_network = Dnn(hidden_units, dropout)\n",
    "            self.nn_final_linear = nn.Linear(hidden_units[-1], 1)\n",
    "        else:\n",
    "            self.fea_num = len(self.dense_feature_cols) + self.sparse_feature_cols[0]['embed_dim']\n",
    "            self.nn_final_linear = nn.Linear(self.fea_num, 1)\n",
    "    def forward(self, x):\n",
    "        dense_inputs, sparse_inputs = x[:, :len(self.dense_feature_cols)], x[:, len(self.dense_feature_cols):]\n",
    "        sparse_inputs = sparse_inputs.long()\n",
    "        sparse_embeds = [self.embed_layers['embed_'+str(i)](sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]\n",
    "        sparse_embeds = torch.stack(sparse_embeds)\n",
    "        sparse_embeds = sparse_embeds.permute((1, 0, 2))\n",
    "        #这里得到embedding向量之后sparse_embeds(None, field_num, embed_dim)\n",
    "        #下面进行两两交叉，注意这时不能加和了，这里两两交叉的结果要进入attention\n",
    "        #两两交叉embedding之后的结果是一个(None, field_num*(field_num-1)/2, embed_dim)\n",
    "        #这里实现的时候采用组合这个技巧\n",
    "        #比如field_num有4个的话，那么组合embedding就是[0,1],[0,2],[0,3],[1,2],[1,3],[2,3]位置的embedding乘积操作\n",
    "        first = []\n",
    "        second = []\n",
    "        for f, s in itertools.combinations(range(sparse_embeds.shape[1]), 2):\n",
    "            first.append(f)\n",
    "            second.append(s)\n",
    "        #取出first位置的embedding，假设field是4的话，就是[0,0,0,1,1,2]位置的embedding\n",
    "        p = sparse_embeds[:, first, :] #(None, field_num*(field_num-1)/2, embed_dim)\n",
    "        q = sparse_embeds[:, second, :] #(None, field_num*(field_num-1)/2, embed_dim)\n",
    "        bi_interaction = p * q\n",
    "        \n",
    "        if self.mode == 'max':\n",
    "            att_out = torch.sum(bi_interaction, dim=1) #(None, embed_dim)\n",
    "        elif self.mode == 'avg':\n",
    "            att_out = torch.mean(bi_interaction, dim=1) #(None, embed_dim)\n",
    "        else:\n",
    "            #注意力网络\n",
    "            att_out = self.attention(bi_interaction)\n",
    "        \n",
    "        #把离散特征和连续特征进行拼接\n",
    "        x = torch.cat([att_out, dense_inputs], dim=-1)\n",
    "        \n",
    "        if not self.useDNN:\n",
    "            outputs = F.sigmoid(self.nn_final_linear(x))\n",
    "        else:\n",
    "            #BatchNormalization\n",
    "            x = self.bn(x)\n",
    "            #deep\n",
    "            dnn_outputs = self.nn_final_linear(self.dnn_network(x))\n",
    "            outputs = F.sigmoid(dnn_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d78330b6-8acc-429b-96e5-248697c1ad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'feat': 'I1'}, {'feat': 'I2'}, {'feat': 'I3'}, {'feat': 'I4'}, {'feat': 'I5'}, {'feat': 'I6'}, {'feat': 'I7'}, {'feat': 'I8'}, {'feat': 'I9'}, {'feat': 'I10'}, {'feat': 'I11'}, {'feat': 'I12'}, {'feat': 'I13'}], [{'feat': 'C1', 'feat_num': 79, 'embed_dim': 8}, {'feat': 'C2', 'feat_num': 252, 'embed_dim': 8}, {'feat': 'C3', 'feat_num': 1293, 'embed_dim': 8}, {'feat': 'C4', 'feat_num': 1043, 'embed_dim': 8}, {'feat': 'C5', 'feat_num': 30, 'embed_dim': 8}, {'feat': 'C6', 'feat_num': 7, 'embed_dim': 8}, {'feat': 'C7', 'feat_num': 1164, 'embed_dim': 8}, {'feat': 'C8', 'feat_num': 39, 'embed_dim': 8}, {'feat': 'C9', 'feat_num': 2, 'embed_dim': 8}, {'feat': 'C10', 'feat_num': 908, 'embed_dim': 8}, {'feat': 'C11', 'feat_num': 926, 'embed_dim': 8}, {'feat': 'C12', 'feat_num': 1239, 'embed_dim': 8}, {'feat': 'C13', 'feat_num': 824, 'embed_dim': 8}, {'feat': 'C14', 'feat_num': 20, 'embed_dim': 8}, {'feat': 'C15', 'feat_num': 819, 'embed_dim': 8}, {'feat': 'C16', 'feat_num': 1159, 'embed_dim': 8}, {'feat': 'C17', 'feat_num': 9, 'embed_dim': 8}, {'feat': 'C18', 'feat_num': 534, 'embed_dim': 8}, {'feat': 'C19', 'feat_num': 201, 'embed_dim': 8}, {'feat': 'C20', 'feat_num': 4, 'embed_dim': 8}, {'feat': 'C21', 'feat_num': 1204, 'embed_dim': 8}, {'feat': 'C22', 'feat_num': 7, 'embed_dim': 8}, {'feat': 'C23', 'feat_num': 12, 'embed_dim': 8}, {'feat': 'C24', 'feat_num': 729, 'embed_dim': 8}, {'feat': 'C25', 'feat_num': 33, 'embed_dim': 8}, {'feat': 'C26', 'feat_num': 554, 'embed_dim': 8}]]\n",
      "(1439, 40) (160, 40) (400, 40)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./data/Criteo_sample.txt')\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=2024)\n",
    "\n",
    "train_label = train_set['label']\n",
    "del train_set['label']\n",
    "test_label = test_set['label']\n",
    "del test_set['label']\n",
    "data_df = pd.concat((train_set, test_set))\n",
    "\n",
    "\n",
    "sparse_feas = [col for col in data_df.columns if col[0] == 'C']\n",
    "dense_feas = [col for col in data_df.columns if col[0] == 'I']\n",
    "\n",
    "data_df[sparse_feas] = data_df[sparse_feas].fillna('-1')\n",
    "data_df[dense_feas] = data_df[dense_feas].fillna(0)\n",
    "\n",
    "def sparseFeature(feat, feat_num, embed_dim=4):\n",
    "    return {'feat':feat, 'feat_num':feat_num, 'embed_dim':embed_dim}\n",
    "def denseFeature(feat):\n",
    "    return {'feat':feat}\n",
    "embed_dim = 8\n",
    "feature_columns = [[denseFeature(feat) for feat in dense_feas]] + [[sparseFeature(feat, len(data_df[feat].unique()), embed_dim=embed_dim) for feat in sparse_feas]]\n",
    "\n",
    "for feat in sparse_feas:\n",
    "    le = LabelEncoder()\n",
    "    data_df[feat] = le.fit_transform(data_df[feat])\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "data_df[dense_feas] = mms.fit_transform(data_df[dense_feas])\n",
    "\n",
    "train = data_df[:train_set.shape[0]]\n",
    "test = data_df[train_set.shape[0]:]\n",
    "\n",
    "train['label'] = train_label\n",
    "test['label'] = test_label\n",
    "\n",
    "train_set, val_set = train_test_split(train, test_size=0.1, random_state=2020)\n",
    "\n",
    "train_set.reset_index(drop=True, inplace=True)\n",
    "val_set.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "print(feature_columns)\n",
    "print(train_set.shape, val_set.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ed7dbd2-b7c4-4059-813e-0cdf392262af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AFM(\n",
       "  (embed_layers): ModuleDict(\n",
       "    (embed_0): Embedding(79, 8)\n",
       "    (embed_1): Embedding(252, 8)\n",
       "    (embed_2): Embedding(1293, 8)\n",
       "    (embed_3): Embedding(1043, 8)\n",
       "    (embed_4): Embedding(30, 8)\n",
       "    (embed_5): Embedding(7, 8)\n",
       "    (embed_6): Embedding(1164, 8)\n",
       "    (embed_7): Embedding(39, 8)\n",
       "    (embed_8): Embedding(2, 8)\n",
       "    (embed_9): Embedding(908, 8)\n",
       "    (embed_10): Embedding(926, 8)\n",
       "    (embed_11): Embedding(1239, 8)\n",
       "    (embed_12): Embedding(824, 8)\n",
       "    (embed_13): Embedding(20, 8)\n",
       "    (embed_14): Embedding(819, 8)\n",
       "    (embed_15): Embedding(1159, 8)\n",
       "    (embed_16): Embedding(9, 8)\n",
       "    (embed_17): Embedding(534, 8)\n",
       "    (embed_18): Embedding(201, 8)\n",
       "    (embed_19): Embedding(4, 8)\n",
       "    (embed_20): Embedding(1204, 8)\n",
       "    (embed_21): Embedding(7, 8)\n",
       "    (embed_22): Embedding(12, 8)\n",
       "    (embed_23): Embedding(729, 8)\n",
       "    (embed_24): Embedding(33, 8)\n",
       "    (embed_25): Embedding(554, 8)\n",
       "  )\n",
       "  (attention): Attention_layer(\n",
       "    (att_w): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (att_dense): Linear(in_features=8, out_features=1, bias=True)\n",
       "  )\n",
       "  (bn): BatchNorm1d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dnn_network): Dnn(\n",
       "    (dnn_network): ModuleList(\n",
       "      (0): Linear(in_features=21, out_features=128, bias=True)\n",
       "      (1): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (nn_final_linear): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#建立模型\n",
    "hidden_units = [128, 64, 32]\n",
    "dnn_dropout = 0.\n",
    "model = AFM(feature_columns, 'att', hidden_units, dropout=dnn_dropout, useDNN=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dd598e1-2ded-4c7b-aa7b-9c05a78d0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_x = train_set.drop('label', axis=1).values\n",
    "trn_y = train_set['label'].values\n",
    "val_x = val_set.drop('label', axis=1).values\n",
    "val_y = val_set['label'].values\n",
    "dl_train_dataset = TensorDataset(torch.tensor(trn_x).float(), torch.tensor(trn_y).float())\n",
    "dl_val_dataset = TensorDataset(torch.tensor(val_x).float(), torch.tensor(val_y).float())\n",
    "\n",
    "dl_train = DataLoader(dl_train_dataset, shuffle=True, batch_size=32)\n",
    "dl_val = DataLoader(dl_val_dataset, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c82cae4-48d8-4d31-85da-2733a8616d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_training.........\n",
      "================================================================\n",
      "[step=10] loss: 0.701, auc: 0.512\n",
      "[step=20] loss: 0.696, auc: 0.517\n",
      "[step=30] loss: 0.691, auc: 0.506\n",
      "[step=40] loss: 0.687, auc: 0.476\n",
      "\n",
      "EPOCH=1, loss=0.685, auc = 0.479, val_loss=0.676, val_auc = 0.579\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.656, auc: 0.522\n",
      "[step=20] loss: 0.648, auc: 0.520\n",
      "[step=30] loss: 0.647, auc: 0.494\n",
      "[step=40] loss: 0.640, auc: 0.487\n",
      "\n",
      "EPOCH=2, loss=0.637, auc = 0.485, val_loss=0.607, val_auc = 0.576\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.590, auc: 0.496\n",
      "[step=20] loss: 0.599, auc: 0.496\n",
      "[step=30] loss: 0.589, auc: 0.505\n",
      "[step=40] loss: 0.583, auc: 0.501\n",
      "\n",
      "EPOCH=3, loss=0.578, auc = 0.502, val_loss=0.547, val_auc = 0.530\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.563, auc: 0.487\n",
      "[step=20] loss: 0.552, auc: 0.529\n",
      "[step=30] loss: 0.542, auc: 0.537\n",
      "[step=40] loss: 0.535, auc: 0.538\n",
      "\n",
      "EPOCH=4, loss=0.528, auc = 0.550, val_loss=0.520, val_auc = 0.574\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.480, auc: 0.597\n",
      "[step=20] loss: 0.501, auc: 0.552\n",
      "[step=30] loss: 0.508, auc: 0.564\n",
      "[step=40] loss: 0.508, auc: 0.569\n",
      "\n",
      "EPOCH=5, loss=0.506, auc = 0.565, val_loss=0.505, val_auc = 0.561\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.459, auc: 0.579\n",
      "[step=20] loss: 0.462, auc: 0.620\n",
      "[step=30] loss: 0.485, auc: 0.615\n",
      "[step=40] loss: 0.486, auc: 0.621\n",
      "\n",
      "EPOCH=6, loss=0.495, auc = 0.611, val_loss=0.491, val_auc = 0.619\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.502, auc: 0.601\n",
      "[step=20] loss: 0.483, auc: 0.622\n",
      "[step=30] loss: 0.480, auc: 0.633\n",
      "[step=40] loss: 0.494, auc: 0.625\n",
      "\n",
      "EPOCH=7, loss=0.491, auc = 0.625, val_loss=0.504, val_auc = 0.581\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.498, auc: 0.662\n",
      "[step=20] loss: 0.489, auc: 0.627\n",
      "[step=30] loss: 0.485, auc: 0.660\n",
      "[step=40] loss: 0.487, auc: 0.661\n",
      "\n",
      "EPOCH=8, loss=0.484, auc = 0.660, val_loss=0.491, val_auc = 0.573\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.471, auc: 0.718\n",
      "[step=20] loss: 0.488, auc: 0.670\n",
      "[step=30] loss: 0.481, auc: 0.657\n",
      "[step=40] loss: 0.476, auc: 0.652\n",
      "\n",
      "EPOCH=9, loss=0.478, auc = 0.661, val_loss=0.493, val_auc = 0.585\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.495, auc: 0.680\n",
      "[step=20] loss: 0.472, auc: 0.673\n",
      "[step=30] loss: 0.466, auc: 0.680\n",
      "[step=40] loss: 0.470, auc: 0.685\n",
      "\n",
      "EPOCH=10, loss=0.475, auc = 0.683, val_loss=0.481, val_auc = 0.609\n",
      "\n",
      "================================================================================\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "def auc(y_pred, y_true):\n",
    "    pred = y_pred.data\n",
    "    y = y_true.data\n",
    "    return roc_auc_score(y, pred)\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "metric_func = auc\n",
    "metric_name = 'auc'\n",
    "epochs = 10\n",
    "log_step_freq = 10\n",
    "\n",
    "dfhistory = pd.DataFrame(columns=['epoch', 'loss', metric_name, 'val_loss', 'val_'+metric_name])\n",
    "\n",
    "print('start_training.........')\n",
    "\n",
    "print('========'*8)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "    metric_sum = 0.0\n",
    "    step = 1\n",
    "    \n",
    "\n",
    "    \n",
    "    for step, (features, labels) in enumerate(dl_train, 1):\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        labels = labels.view(-1, 1)\n",
    "        \n",
    "        # 正向传播\n",
    "        predictions = model(features);\n",
    "        loss = loss_func(predictions, labels)\n",
    "        try:\n",
    "            metric = metric_func(predictions, labels)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印batch级别日志\n",
    "        loss_sum += loss.item()\n",
    "        metric_sum += metric.item()\n",
    "        if step % log_step_freq == 0:\n",
    "            print((\"[step=%d] loss: %.3f, \" + metric_name + \": %.3f\") % (step, loss_sum/step, metric_sum/step));\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    val_metric_sum = 0.0\n",
    "    val_step = 1\n",
    "    \n",
    "    for val_step, (features, labels) in enumerate(dl_val, 1):\n",
    "        labels = labels.view(-1, 1)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(features)\n",
    "            val_loss = loss_func(predictions, labels)\n",
    "            try:\n",
    "                val_metric = metric_func(predictions, labels)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        val_loss_sum += val_loss.item()\n",
    "        val_metric_sum += val_metric.item()\n",
    "    \n",
    "    # 记录日志\n",
    "    info = (epoch, loss_sum/step, metric_sum/step, val_loss_sum/val_step, val_metric_sum/val_step)\n",
    "    dfhistory.loc[epoch-1] = info\n",
    "    \n",
    "    # 打印日志\n",
    "    print((\"\\nEPOCH=%d, loss=%.3f, \" + metric_name + \" = %.3f, val_loss=%.3f, \" + \"val_\" + metric_name + \" = %.3f\") %info)\n",
    "    print('\\n' + '=========='* 8)\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bca6745c-41c1-4b65-8b7d-5ba99aa2e885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc:0.666 test_acc:0.765\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_x = test.drop('label', axis=1).values\n",
    "test_y = test['label'].values\n",
    "y_pred_probs = model(torch.tensor(test_x).float())\n",
    "y_pred = torch.where(y_pred_probs>0.5, torch.ones_like(y_pred_probs), torch.zeros_like(y_pred_probs))\n",
    "\n",
    "test_auc = roc_auc_score(test_y, y_pred_probs.data.numpy())\n",
    "test_acc = accuracy_score(test_y, y_pred.data.numpy())\n",
    "print('test_auc:%.3f test_acc:%.3f'%(test_auc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbe546-4d19-4599-afdd-1e03d10a6196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
