{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595d52d3-6c0b-401f-b2ca-96d836fe7920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c7abb7-12f2-4629-9ce5-6221fbdfb90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_layer(nn.Module):\n",
    "    #自定义Attenion层，其实就是一个全连接神经网络\n",
    "    def __init__(self, att_hidden_units, activation='sigmoid'):\n",
    "        super(Attention_layer, self).__init__()\n",
    "        self.att_layer = nn.ModuleList([nn.Linear(layer[0], layer[1]) for layer in list(zip(att_hidden_units[:-1], att_hidden_units[1:]))])\n",
    "        self.att_final_layer = nn.Linear(att_hidden_units[-1], 1)\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        这里的inputs包含四个部分:[item_embed, seq_embed, seq_embed, mask]\n",
    "        item_embed:这个是候选商品的embedding向量，维度是(None, embedding_dim * behavior_num) #behavior_num能表示用户行为的特征个数 这里是1，所以(None, embed_dim)\n",
    "        mask: 维度是(None, max_len) 这个里面每一行是[False, False, True, True, ...]的形式，False的长度表示样本填充的那部分\n",
    "        \"\"\"\n",
    "        q, k, v, key_masks = inputs\n",
    "        q = q.repeat(1, k.shape[1])   #(None. max_len*embedding) #沿着k.shape[1]的维度复制 每个历史行为都要和当前的商品计算相似关系\n",
    "        q = torch.reshape(q, (-1, k.shape[1], k.shape[2]))   #(None, max_len, embedding_dim)\n",
    "        \n",
    "        #q, k, out product should concat\n",
    "        info = torch.cat([q, k, q-k, q*k], dim=-1) #(None, max_len, 4*embedding_dim)\n",
    "        \n",
    "        #n层全连接\n",
    "        for linear in self.att_layer:\n",
    "            info = linear(info)\n",
    "            info = F.relu(info)\n",
    "        outputs = self.att_final_layer(info)   #(None, max_len, 1)\n",
    "#         print(outputs.shape)\n",
    "        outputs = torch.squeeze(outputs, dim=-1) #(None, max_len)\n",
    "#         print(outputs.shape)\n",
    "        \n",
    "        #mask 把每个行为序列填充的那部分替换成很小的一个值\n",
    "        paddings = torch.ones_like(outputs) * (-2**32+1)    #(None, max_len)这个就是之前填充的那个地方，我们补一个很小的值\n",
    "        outputs = torch.where(key_masks==0, paddings, outputs)\n",
    "#         print(outputs.shape)\n",
    "        \n",
    "        #softmax\n",
    "        outputs = F.softmax(outputs, dim=1) #(None, max_len)\n",
    "        outputs = torch.unsqueeze(outputs, 1) #(None, 1, max_len)\n",
    "        \n",
    "        outputs = torch.matmul(outputs, v) #三维矩阵相乘，(None, 1, max_len)*(None, max_len, embed_dim) = (None,1,embed_dim)\n",
    "        outputs = torch.squeeze(outputs, dim=1)  #(None,embed_dim)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ca3650c-a16d-4f24-8f9d-0150e26d604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dice(nn.Module):\n",
    "    def __init__(self, fea_num):\n",
    "        super(Dice, self).__init__()\n",
    "        self.fea_num = fea_num\n",
    "        self.bn = nn.BatchNorm1d(fea_num)\n",
    "        self.alpha = nn.Parameter(torch.randn(1))\n",
    "        nn.init.uniform_(self.alpha, 0, 1)\n",
    "    def forward(self, x):\n",
    "        x_normed = self.bn(x)\n",
    "        x_p = torch.sigmoid(x_normed)\n",
    "        \n",
    "        return self.alpha * (1.0-x_p) * x + x_p * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65cfed12-8024-4465-9648-04d3e30f48f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIN(nn.Module):\n",
    "    def __init__(self, feature_columns, behavior_feature_list, att_hidden_units=(80, 40), ffn_hidden_units=(80, 40), att_activation='sigmoid',\n",
    "                ffn_activation='prelu', maxlen=40, dnn_dropout=0., embed_reg=1e-4):\n",
    "        \"\"\"\n",
    "        feature_columns:列表，[dense_feature_columns, sparse_feature_columns], dense_feature_columns是[{'feat':'feat_name'}], 而sparse_feature_columns是[{'feat':'feat_name', 'feat_num':'nunique', 'embed_dim'}]\n",
    "        behavior_feature_list:列表，能表示用户历史行为的特征，比如商品id,店铺id['item', 'cat']\n",
    "        att_hidden_units:注意力层的隐藏单元个数。可以是一个列表或者元组，注意注意力层是一个全连接网络\n",
    "        ffn_hidden_units:全连接层的隐藏单元个数和层数，可以是一个列表或者元组\n",
    "        att_activation:激活单元的名称，字符串\n",
    "        ffn_activation:激活单元的名称，用'prelu'或者'Dice'\n",
    "        maxlen:标量，用户历史行为序列的最大长度\n",
    "        dropout:标量，失活率\n",
    "        embed_reg:标量，正则系数\n",
    "        \"\"\"\n",
    "        super(DIN, self).__init__() #初始化网络\n",
    "        self.maxlen = maxlen\n",
    "        self.dense_feature_columns, self.sparse_feature_columns = feature_columns #将连续特征和离散特征分别取出来，因为这两者后期的处理不同\n",
    "        \n",
    "        #len\n",
    "        self.other_sparse_len = len(self.sparse_feature_columns) - len(behavior_feature_list) #这个other_sparse_len就是离散特征中去掉了能表示用户行为的特征列\n",
    "        self.dense_len = len(self.dense_feature_columns)\n",
    "        self.behavior_num = len(behavior_feature_list)\n",
    "        \n",
    "        self.fea_num = 0\n",
    "        if self.other_sparse_len > 0 or self.dense_len > 0:\n",
    "            self.fea_num = self.dense_len + self.other_sparse_len + self.sparse_feature_columns[0]['embed_dim'] * 2\n",
    "        else:\n",
    "            self.fea_num = self.sparse_feature_columns[0]['embed_dim'] * 2\n",
    "        #embedding层，这里分成两部分的embedding，第一部分是普通的离散特征，第二部分是能表示用户历史行为的离散特征，这一块后面要进注意力和当前的商品计算相关性\n",
    "        self.embed_sparse_layers = nn.ModuleList([nn.Embedding(num_embeddings=feat['feat_num'], embedding_dim=feat['embed_dim']) for feat in self.sparse_feature_columns if feat['feat'] not in behavior_feature_list])\n",
    "        \n",
    "        #behavior embedding layers, item id and category id\n",
    "        self.embed_seq_layers = nn.ModuleList([nn.Embedding(num_embeddings=feat['feat_num'], embedding_dim=feat['embed_dim']) for feat in self.sparse_feature_columns if feat['feat'] in behavior_feature_list])\n",
    "        \n",
    "        #注意力机制\n",
    "        att_hidden_units.insert(0, self.sparse_feature_columns[0]['embed_dim'] * 4)\n",
    "        self.attention_layer = Attention_layer(att_hidden_units, att_activation)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(self.fea_num)\n",
    "        \n",
    "        #全连接网络\n",
    "        self.ffn_activation = ffn_activation\n",
    "        ffn_hidden_units.insert(0, self.fea_num)\n",
    "        self.ffn = nn.ModuleList(nn.Linear(layer[0], layer[1]) for layer in list(zip(ffn_hidden_units[:-1], ffn_hidden_units[1:])))\n",
    "        self.dropout = nn.Dropout(dnn_dropout)\n",
    "        self.final_fnn_layer = nn.Linear(ffn_hidden_units[-1], 1)\n",
    "        \n",
    "        #dice\n",
    "        self.dice_fea_nums = ffn_hidden_units[1:]\n",
    "        self.dice = nn.ModuleList(Dice(dice_fea_num) for dice_fea_num in self.dice_fea_nums)\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs:[dense_inputs, sparse_input, seq_input, item_input],第二部分是离散型的特征输入，第三部分是用户的历史行为，第四部分是当前商品的输入\n",
    "        dense_input:连续型的特征输入,维度是(None, dense_len)\n",
    "        sparse_input:离散型的特征输入,维度是(None, other_sparse_len)\n",
    "        seq_input:用户的历史行为xulie(None, maxlen, behavior_len)\n",
    "        item_input:当前的候选商品序列(None, behavior_len)\n",
    "        \"\"\"\n",
    "        dense_inputs, sparse_inputs, seq_inputs, item_inputs = inputs\n",
    "        sparse_inputs = sparse_inputs.long()\n",
    "        seq_inputs = seq_inputs.long()\n",
    "        item_inputs = item_inputs.long()\n",
    "        \n",
    "        #attention ----> mask, if the element of seq_inputs is equal 0, it must be filled in \n",
    "        mask = torch.tensor(seq_inputs[:,:,0]!=0, dtype=torch.float32)  #(None, maxlen)类型转换函数，把seq_input中不等于0的值转成float32\n",
    "        #这个函数的作用就是每一行样本中，不为0的值返回1，为0的值返回0，这样就把填充的那部分值都给标记了出来\n",
    "        \n",
    "        #下面把连续型特征和行为无关的离散型特征拼到一起先\n",
    "        other_info = dense_inputs  #(None, dense_len)\n",
    "        for i in range(self.other_sparse_len):\n",
    "            other_info = torch.cat([other_info, self.embed_sparse_layers[i](sparse_inputs[:,i])], dim=-1) #(None, dense_len+other_sparse_len)\n",
    "        \n",
    "        #下面把候选的商品和用户行为商品也各自的拼接起来\n",
    "        seq_embed = torch.cat([self.embed_seq_layers[i](seq_inputs[:,:,i]) for i in range(self.behavior_num)], dim=-1)#(None, max_len, embed_dim)\n",
    "        item_embed = torch.cat([self.embed_seq_layers[i](item_inputs[:, i]) for i in range(self.behavior_num)],dim=-1) #(None, embed_dim)\n",
    "        \n",
    "        #下面进行attention_layer的计算\n",
    "        user_info = self.attention_layer([item_embed, seq_embed, seq_embed, mask]) #(None, embed_dim)\n",
    "        \n",
    "        #所有特征拼接起来\n",
    "        if self.dense_len > 0 or self.other_sparse_len > 0:\n",
    "            info_all = torch.cat([user_info, item_embed, other_info], dim=-1) #(None, dense_len+other_sparse_len+embed_dim+embed_dim)\n",
    "        else:\n",
    "            info_all = torch.cat([user_info, item_embed], dim=-1) #(None, embed_dim+embed_dim)\n",
    "        \n",
    "        info_all = self.bn(info_all)\n",
    "        \n",
    "        #ffn\n",
    "        dice_i = 0\n",
    "        for linear in self.ffn:\n",
    "            info_all = linear(info_all)\n",
    "            if self.ffn_activation == 'prelu':\n",
    "                activation_func = nn.PReLU(num_parameters=1)\n",
    "                info_all = activation_func(info_all)\n",
    "            else:\n",
    "                info_all = self.dice[dice_i](info_all)\n",
    "                dice_i += 1\n",
    "        info_all = self.dropout(info_all)\n",
    "        outputs = torch.sigmoid(self.final_fnn_layer(info_all))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "43476791-b393-4bbf-9b2e-bd18a92011bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparseFeature(feat, feat_num, embed_dim=4):\n",
    "    return {'feat': feat, 'feat_num': feat_num, 'embed_dim': embed_dim}\n",
    "\n",
    "\n",
    "def denseFeature(feat):\n",
    "    return {'feat': feat}\n",
    "\n",
    "\n",
    "def create_amazon_electronic_dataset(file, embed_dim=8, maxlen=40):\n",
    "    print('==========Data Preprocess Start============')\n",
    "    with open('./data/remap.pkl', 'rb') as f:\n",
    "        reviews_df = pickle.load(f)\n",
    "        cate_list = pickle.load(f)\n",
    "        user_count, item_count, cate_count, example_count = pickle.load(f)\n",
    "\n",
    "    reviews_df = reviews_df\n",
    "    reviews_df.columns = ['user_id', 'item_id', 'time']\n",
    "\n",
    "    train_data, val_data, test_data = [], [], []\n",
    "\n",
    "    for user_id, hist in tqdm(reviews_df.groupby('user_id')):\n",
    "        pos_list = hist['item_id'].tolist()\n",
    "\n",
    "        def gen_neg():\n",
    "            neg = pos_list[0]\n",
    "            while neg in pos_list:\n",
    "                neg = random.randint(0, item_count - 1)\n",
    "            return neg\n",
    "\n",
    "        neg_list = [gen_neg() for i in range(len(pos_list))]\n",
    "        hist = []\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist.append([pos_list[i - 1], cate_list[pos_list[i-1]]])\n",
    "            hist_i = hist.copy()\n",
    "            if i == len(pos_list) - 1:\n",
    "                test_data.append([hist_i, [pos_list[i], cate_list[pos_list[i]]], 1])\n",
    "                test_data.append([hist_i, [neg_list[i], cate_list[neg_list[i]]], 0])\n",
    "                \n",
    "            elif i == len(pos_list) - 2:\n",
    "                val_data.append([hist_i, [pos_list[i], cate_list[pos_list[i]]], 1])\n",
    "                val_data.append([hist_i, [neg_list[i], cate_list[neg_list[i]]], 0])\n",
    "\n",
    "            else:\n",
    "                train_data.append([hist_i, [pos_list[i], cate_list[pos_list[i]]], 1])\n",
    "                train_data.append([hist_i, [neg_list[i], cate_list[neg_list[i]]], 0])\n",
    "                \n",
    "\n",
    "    # feature columns\n",
    "    feature_columns = [[],\n",
    "                       [sparseFeature('item_id', item_count, embed_dim),\n",
    "                        ]]  # sparseFeature('cate_id', cate_count, embed_dim)\n",
    "\n",
    "    # behavior\n",
    "    behavior_list = ['item_id']\n",
    "\n",
    "    # shuffle\n",
    "    random.shuffle(train_data)\n",
    "    random.shuffle(val_data)\n",
    "    random.shuffle(test_data)\n",
    "\n",
    "    # create dataframe\n",
    "    train = pd.DataFrame(train_data, columns=['hist', 'target_item', 'label'])\n",
    "    val = pd.DataFrame(val_data, columns=['hist', 'target_item', 'label'])\n",
    "    test = pd.DataFrame(test_data, columns=['hist', 'target_item', 'label'])\n",
    "\n",
    "    # if no dense or sparse features, can fill with 0\n",
    "    print('==================Padding===================')\n",
    "    train_X = [np.array([0.] * len(train)), np.array([0] * len(train)),\n",
    "               pad_sequences(train['hist'], maxlen=maxlen),\n",
    "               np.array(train['target_item'].tolist())]\n",
    "    train_y = train['label'].values\n",
    "    val_X = [np.array([0] * len(val)), np.array([0] * len(val)),\n",
    "             pad_sequences(val['hist'], maxlen=maxlen),\n",
    "             np.array(val['target_item'].tolist())]\n",
    "    val_y = val['label'].values\n",
    "    test_X = [np.array([0] * len(test)), np.array([0] * len(test)),\n",
    "              pad_sequences(test['hist'], maxlen=maxlen),\n",
    "              np.array(test['target_item'].tolist())]\n",
    "    test_y = test['label'].values\n",
    "    test = (test_X, test_y)\n",
    "    print('============Data Preprocess End=============')\n",
    "    return feature_columns, behavior_list, (train_X, train_y), (val_X, val_y), (test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "34fe3aff-3bb1-4357-9a30-c107baf52992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Data Preprocess Start============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192403/192403 [00:13<00:00, 14197.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================Padding===================\n",
      "============Data Preprocess End=============\n"
     ]
    }
   ],
   "source": [
    "feature_columns, behavior_list, (train_X, train_y), (val_X, val_y), (test_X, test_y) = create_amazon_electronic_dataset('./data/remap.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1ab4665a-fc27-404f-a31e-2994c2d49fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DIN(\n",
       "  (embed_sparse_layers): ModuleList()\n",
       "  (embed_seq_layers): ModuleList(\n",
       "    (0): Embedding(63001, 8)\n",
       "  )\n",
       "  (attention_layer): Attention_layer(\n",
       "    (att_layer): ModuleList(\n",
       "      (0): Linear(in_features=32, out_features=80, bias=True)\n",
       "      (1): Linear(in_features=80, out_features=40, bias=True)\n",
       "    )\n",
       "    (att_final_layer): Linear(in_features=40, out_features=1, bias=True)\n",
       "  )\n",
       "  (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (ffn): ModuleList(\n",
       "    (0): Linear(in_features=16, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (final_fnn_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (dice): ModuleList(\n",
       "    (0): Dice(\n",
       "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Dice(\n",
       "      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Dice(\n",
       "      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#建立模型\n",
    "maxlen = 40\n",
    "embed_dim = 8\n",
    "att_hidden_units = [80, 40]\n",
    "ffn_hidden_units = [256, 128, 64]\n",
    "dnn_dropout = 0.5\n",
    "att_activation = 'sigmoid'\n",
    "ffn_activation = 'dice'\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "model = DIN(feature_columns, behavior_list, att_hidden_units, ffn_hidden_units, att_activation, ffn_activation, maxlen, dnn_dropout)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b5eacf8-f454-493b-9a80-8b4a2d872e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train_dataset = TensorDataset(torch.tensor(train_X[0].reshape(-1,1)).float(), torch.tensor(train_X[1].reshape(-1,1)).float(), torch.tensor(train_X[2]).float(), torch.tensor(train_X[3]).float(), torch.tensor(train_y).float())\n",
    "dl_val_dataset = TensorDataset(torch.tensor(val_X[0].reshape(-1,1)).float(), torch.tensor(val_X[1].reshape(-1,1)).float(), torch.tensor(val_X[2]).float(), torch.tensor(val_X[3]).float(), torch.tensor(val_y).float())\n",
    "\n",
    "dl_train = DataLoader(dl_train_dataset, shuffle=True, batch_size=20000)\n",
    "dl_val = DataLoader(dl_val_dataset, shuffle=True, batch_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a19b1157-e662-4bf4-8714-9870ec69a7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_training.........\n",
      "================================================================\n",
      "[step=10] loss: 0.694, auc: 0.502\n",
      "[step=20] loss: 0.694, auc: 0.502\n",
      "[step=30] loss: 0.693, auc: 0.503\n",
      "[step=40] loss: 0.693, auc: 0.505\n",
      "[step=50] loss: 0.693, auc: 0.507\n",
      "[step=60] loss: 0.693, auc: 0.509\n",
      "[step=70] loss: 0.693, auc: 0.511\n",
      "[step=80] loss: 0.693, auc: 0.513\n",
      "[step=90] loss: 0.693, auc: 0.516\n",
      "[step=100] loss: 0.693, auc: 0.518\n",
      "[step=110] loss: 0.692, auc: 0.521\n",
      "\n",
      "EPOCH=1, loss=0.692, auc = 0.521, val_loss=0.688, val_auc = 0.558\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.688, auc: 0.555\n",
      "[step=20] loss: 0.688, auc: 0.559\n",
      "[step=30] loss: 0.687, auc: 0.563\n",
      "[step=40] loss: 0.686, auc: 0.566\n",
      "[step=50] loss: 0.685, auc: 0.570\n",
      "[step=60] loss: 0.685, auc: 0.573\n",
      "[step=70] loss: 0.684, auc: 0.576\n",
      "[step=80] loss: 0.683, auc: 0.579\n",
      "[step=90] loss: 0.682, auc: 0.582\n",
      "[step=100] loss: 0.681, auc: 0.585\n",
      "[step=110] loss: 0.680, auc: 0.588\n",
      "\n",
      "EPOCH=2, loss=0.680, auc = 0.589, val_loss=0.667, val_auc = 0.626\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.666, auc: 0.628\n",
      "[step=20] loss: 0.665, auc: 0.631\n",
      "[step=30] loss: 0.663, auc: 0.635\n",
      "[step=40] loss: 0.662, auc: 0.637\n",
      "[step=50] loss: 0.660, auc: 0.640\n",
      "[step=60] loss: 0.659, auc: 0.643\n",
      "[step=70] loss: 0.658, auc: 0.645\n",
      "[step=80] loss: 0.656, auc: 0.648\n",
      "[step=90] loss: 0.655, auc: 0.651\n",
      "[step=100] loss: 0.653, auc: 0.653\n",
      "[step=110] loss: 0.652, auc: 0.655\n",
      "\n",
      "EPOCH=3, loss=0.652, auc = 0.656, val_loss=0.635, val_auc = 0.680\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.632, auc: 0.690\n",
      "[step=20] loss: 0.630, auc: 0.692\n",
      "[step=30] loss: 0.628, auc: 0.694\n",
      "[step=40] loss: 0.627, auc: 0.696\n",
      "[step=50] loss: 0.626, auc: 0.698\n",
      "[step=60] loss: 0.625, auc: 0.699\n",
      "[step=70] loss: 0.623, auc: 0.701\n",
      "[step=80] loss: 0.622, auc: 0.703\n",
      "[step=90] loss: 0.620, auc: 0.705\n",
      "[step=100] loss: 0.619, auc: 0.707\n",
      "[step=110] loss: 0.618, auc: 0.708\n",
      "\n",
      "EPOCH=4, loss=0.617, auc = 0.709, val_loss=0.605, val_auc = 0.720\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.598, auc: 0.733\n",
      "[step=20] loss: 0.597, auc: 0.735\n",
      "[step=30] loss: 0.596, auc: 0.736\n",
      "[step=40] loss: 0.595, auc: 0.737\n",
      "[step=50] loss: 0.594, auc: 0.738\n",
      "[step=60] loss: 0.593, auc: 0.739\n",
      "[step=70] loss: 0.592, auc: 0.740\n",
      "[step=80] loss: 0.591, auc: 0.742\n",
      "[step=90] loss: 0.590, auc: 0.742\n",
      "[step=100] loss: 0.589, auc: 0.744\n",
      "[step=110] loss: 0.588, auc: 0.745\n",
      "\n",
      "EPOCH=5, loss=0.588, auc = 0.745, val_loss=0.583, val_auc = 0.745\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.574, auc: 0.762\n",
      "[step=20] loss: 0.573, auc: 0.762\n",
      "[step=30] loss: 0.573, auc: 0.763\n",
      "[step=40] loss: 0.572, auc: 0.763\n",
      "[step=50] loss: 0.571, auc: 0.764\n",
      "[step=60] loss: 0.570, auc: 0.765\n",
      "[step=70] loss: 0.569, auc: 0.765\n",
      "[step=80] loss: 0.569, auc: 0.766\n",
      "[step=90] loss: 0.568, auc: 0.767\n",
      "[step=100] loss: 0.567, auc: 0.767\n",
      "[step=110] loss: 0.567, auc: 0.768\n",
      "\n",
      "EPOCH=6, loss=0.567, auc = 0.768, val_loss=0.569, val_auc = 0.759\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.556, auc: 0.779\n",
      "[step=20] loss: 0.555, auc: 0.780\n",
      "[step=30] loss: 0.554, auc: 0.781\n",
      "[step=40] loss: 0.554, auc: 0.781\n",
      "[step=50] loss: 0.554, auc: 0.781\n",
      "[step=60] loss: 0.554, auc: 0.781\n",
      "[step=70] loss: 0.553, auc: 0.782\n",
      "[step=80] loss: 0.553, auc: 0.782\n",
      "[step=90] loss: 0.553, auc: 0.783\n",
      "[step=100] loss: 0.552, auc: 0.783\n",
      "[step=110] loss: 0.552, auc: 0.783\n",
      "\n",
      "EPOCH=7, loss=0.552, auc = 0.783, val_loss=0.560, val_auc = 0.769\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.545, auc: 0.790\n",
      "[step=20] loss: 0.545, auc: 0.791\n",
      "[step=30] loss: 0.544, auc: 0.792\n",
      "[step=40] loss: 0.544, auc: 0.792\n",
      "[step=50] loss: 0.544, auc: 0.792\n",
      "[step=60] loss: 0.543, auc: 0.793\n",
      "[step=70] loss: 0.543, auc: 0.793\n",
      "[step=80] loss: 0.543, auc: 0.793\n",
      "[step=90] loss: 0.543, auc: 0.793\n",
      "[step=100] loss: 0.543, auc: 0.793\n",
      "[step=110] loss: 0.542, auc: 0.793\n",
      "\n",
      "EPOCH=8, loss=0.542, auc = 0.794, val_loss=0.556, val_auc = 0.775\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.538, auc: 0.799\n",
      "[step=20] loss: 0.537, auc: 0.799\n",
      "[step=30] loss: 0.536, auc: 0.800\n",
      "[step=40] loss: 0.536, auc: 0.800\n",
      "[step=50] loss: 0.536, auc: 0.800\n",
      "[step=60] loss: 0.536, auc: 0.800\n",
      "[step=70] loss: 0.536, auc: 0.800\n",
      "[step=80] loss: 0.536, auc: 0.801\n",
      "[step=90] loss: 0.535, auc: 0.801\n",
      "[step=100] loss: 0.536, auc: 0.801\n",
      "[step=110] loss: 0.535, auc: 0.801\n",
      "\n",
      "EPOCH=9, loss=0.536, auc = 0.801, val_loss=0.552, val_auc = 0.779\n",
      "\n",
      "================================================================================\n",
      "[step=10] loss: 0.530, auc: 0.806\n",
      "[step=20] loss: 0.531, auc: 0.806\n",
      "[step=30] loss: 0.531, auc: 0.806\n",
      "[step=40] loss: 0.530, auc: 0.806\n",
      "[step=50] loss: 0.530, auc: 0.806\n",
      "[step=60] loss: 0.531, auc: 0.806\n",
      "[step=70] loss: 0.530, auc: 0.806\n",
      "[step=80] loss: 0.531, auc: 0.806\n",
      "[step=90] loss: 0.531, auc: 0.806\n",
      "[step=100] loss: 0.530, auc: 0.806\n",
      "[step=110] loss: 0.530, auc: 0.806\n",
      "\n",
      "EPOCH=10, loss=0.530, auc = 0.806, val_loss=0.550, val_auc = 0.782\n",
      "\n",
      "================================================================================\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "def auc(y_pred, y_true):\n",
    "    pred = y_pred.data\n",
    "    y = y_true.data\n",
    "    return roc_auc_score(y, pred)\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "metric_func = auc\n",
    "metric_name = 'auc'\n",
    "epochs = 10\n",
    "log_step_freq = 10\n",
    "\n",
    "dfhistory = pd.DataFrame(columns=['epoch', 'loss', metric_name, 'val_loss', 'val_'+metric_name])\n",
    "\n",
    "print('start_training.........')\n",
    "\n",
    "print('========'*8)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "    metric_sum = 0.0\n",
    "    step = 1\n",
    "    \n",
    "    for step, (dense_input, sparse_input, behavior_input, target_item, labels) in enumerate(dl_train, 1):\n",
    "        labels = labels.view(-1, 1)\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 正向传播\n",
    "        predictions = model([dense_input, sparse_input, behavior_input, target_item])\n",
    "        loss = loss_func(predictions, labels)\n",
    "        try:\n",
    "            metric = metric_func(predictions, labels)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印batch级别日志\n",
    "        loss_sum += loss.item()\n",
    "        metric_sum += metric.item()\n",
    "        if step % log_step_freq == 0:\n",
    "            print((\"[step=%d] loss: %.3f, \" + metric_name + \": %.3f\") % (step, loss_sum/step, metric_sum/step));\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    val_metric_sum = 0.0\n",
    "    val_step = 1\n",
    "    \n",
    "    for val_step, (dense_input, sparse_input, behavior_input, target_item, labels) in enumerate(dl_val, 1):\n",
    "        labels = labels.view(-1, 1)\n",
    "        with torch.no_grad():\n",
    "            predictions = model([dense_input, sparse_input, behavior_input, target_item])\n",
    "            val_loss = loss_func(predictions, labels)\n",
    "            try:\n",
    "                val_metric = metric_func(predictions, labels)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        val_loss_sum += val_loss.item()\n",
    "        val_metric_sum += val_metric.item()\n",
    "    \n",
    "    # 记录日志\n",
    "    info = (epoch, loss_sum/step, metric_sum/step, val_loss_sum/val_step, val_metric_sum/val_step)\n",
    "    dfhistory.loc[epoch-1] = info\n",
    "    \n",
    "    # 打印日志\n",
    "    print((\"\\nEPOCH=%d, loss=%.3f, \" + metric_name + \" = %.3f, val_loss=%.3f, \" + \"val_\" + metric_name + \" = %.3f\") %info)\n",
    "    \n",
    "    print('\\n' + '=========='* 8)\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542bf899-93e9-4bba-8db9-30b6215d2578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
